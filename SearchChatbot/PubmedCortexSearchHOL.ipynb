{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "4denbo5e27zzjifa75v3",
   "authorId": "5047391741011",
   "authorName": "FH_LAB1",
   "authorEmail": "",
   "sessionId": "cbdd1c04-1a83-4130-b8f1-f464801c415f",
   "lastEditTime": 1740713986498
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bd41b6-27ac-4a8e-a0ad-65cd9e0572df",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# ❄️ Snowflake Chat with NCBI PubMed Articles ❄️"
  },
  {
   "cell_type": "markdown",
   "id": "d9b5ca11-ea70-4dee-87fa-cfa00a8b4f5a",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "In this Hands on Lab we will load public NCBI Pubmed articles into snowflake and set up Cortex Search Service on the articles so we can leverage Cortex LLMs to asks questions in natural laguage about the articles.\n\nDO NOT RUN ALL cells, we will step throught each cell one at a time. \n\nAdapted version of this quickstart: https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html?index=..%2F..index#0 \n\n"
  },
  {
   "cell_type": "markdown",
   "id": "39508ca6-f904-45fa-97ac-03c0abfcd4f6",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "1. Search and add snowflake.core and langchain from Packages Dropdown on the top right corner\n\n2. Start the Notebook session"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "source": "st.image( \"https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/img/1d96fe59a89a3ac5.png\", width=1000)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "cell3",
    "collapsed": false
   },
   "source": "/**************************\n Set the context - update the database\n**************************/\nuse database fh_pmc_data1;\nuse schema pmc_oa_opendata;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ef5191e4-100a-4d80-b9dd-ac5bc6fb3262",
   "metadata": {
    "language": "sql",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "\n--https://www.ncbi.nlm.nih.gov/pmc/tools/pmcaws/\n/**************************\nPubmed article TEXT files are available in a public s3\nAssociated metadata about the articles available are in a metadata file\n**************************/\ncreate or replace stage pmc_oa_comm_raw url = 's3://pmc-oa-opendata/oa_comm';\n\ncreate \nor replace file format my_csv_format type = csv skip_header=1;\n\n\n--Key,ETag,Article Citation,AccessionID,Last Updated UTC (YYYY-MM-DD HH:MM:SS),PMID,License,Retracted\n\nCREATE \nor replace TABLE oa_comm_metadata (\nKey text,\netag text,\narticle_citation text,\naccessionid text,\nlast_updated_utc  text ,\npmid text, \nlicense text,\nretracted text\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42cc7dc6-df56-4532-acde-df206a30d91e",
   "metadata": {
    "language": "sql",
    "name": "cell8",
    "collapsed": false
   },
   "outputs": [],
   "source": "/**************************\nLoad the metadata file into snowflake table\n**************************/\n\ncopy into oa_comm_metadata FROM @pmc_oa_comm_raw/txt/metadata/csv/oa_comm.filelist.csv \nfile_format= 'my_csv_format'\nON_ERROR= 'CONTINUE';\n\nselect * from oa_comm_metadata order by accessionid asc limit 20;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "952494b0-5d7b-4806-a371-f60e1ec41802",
   "metadata": {
    "language": "sql",
    "name": "cell9",
    "collapsed": false
   },
   "outputs": [],
   "source": "/**************************\nSnowpark Python function to chunk files, chunk size configurable\n**************************/\n\ncreate or replace function text_chunker(file_url string)\nreturns table (chunk varchar)\nlanguage python\nruntime_version = '3.9'\nhandler = 'text_chunker'\npackages = ('snowflake-snowpark-python','langchain')\nas\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.files import SnowflakeFile\nimport io\nimport logging\nimport pandas as pd\n\nclass text_chunker:\n\n    def read_txt(self, file_url: str) -> str:\n    \n        logger = logging.getLogger(\"udf_logger\")\n        logger.info(f\"Opening file {file_url}\")\n    \n        with SnowflakeFile.open(file_url, 'rb') as f:\n            buffer = io.BytesIO(f.readall())\n\n        try:\n            text = buffer.getvalue().decode('utf-8').replace('\\n', ' ').replace('\\0', ' ')\n\n        except Exception as e:\n            logger.warn(f\"Unable to extract from file {file_url}: {str(e)}\")\n            text = \"Unable to Extract\"\n        \n        return text\n\n    def process(self, file_url: str):\n\n        text = self.read_txt(file_url)\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 4000, #Adjust this as you see fit\n            chunk_overlap  = 400, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(text)\n        df = pd.DataFrame(chunks, columns=['chunk'])\n        \n        yield from df.itertuples(index=False, name=None)\n$$;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea032456-4e04-497e-9819-ab5850a39679",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "Note that if you have PDF documents you can alternatively use Cortex AI Task Functions [Parse Document](https://docs.snowflake.com/en/user-guide/snowflake-cortex/parse-document) and [Split Text Recursive Character](https://docs.snowflake.com/en/sql-reference/functions/split_text_recursive_character-snowflake-cortex). Instead of writing a chunking snowpark function you can call these functions on your staged pdf docs. "
  },
  {
   "cell_type": "code",
   "id": "da6e5cce-9cb1-4d04-b4e5-ccaed464353e",
   "metadata": {
    "language": "sql",
    "name": "cell10",
    "collapsed": false
   },
   "outputs": [],
   "source": "--Table to load text from pmc articles in AWS external stage \ncreate or replace TABLE PMC_OA_CHUNKS_TABLE ( \n    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the file\n    ABS_PATH  VARCHAR(16777216), -- Path for the file\n    Etag VARCHAR(16777216),\n    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n    CHUNK VARCHAR(16777216)-- Piece of text\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29f36f94-f5cb-4369-b111-2b9d45b2726c",
   "metadata": {
    "language": "sql",
    "name": "cell20",
    "collapsed": false
   },
   "outputs": [],
   "source": "/**************************\nLet's see the text_chunker function in action \nThis is what we will insert in the chunks table\n**************************/\n\nwith oa_comm as (select array_to_string(array_slice(split(key, '/'),1,4), '/') as relative_path , etag \nfrom oa_comm_metadata where etag not in (select etag from PMC_OA_chunks_table ) limit 10)\nselect \n    relative_path, \n    get_absolute_path(@pmc_oa_comm_raw, relative_path) as abs_path,\n    etag,\n    build_scoped_file_url(@pmc_oa_comm_raw, relative_path) as scoped_file_url,\n    func.chunk as chunk\nfrom \n    oa_comm,\n    TABLE(text_chunker(build_scoped_file_url(@pmc_oa_comm_raw, relative_path))) as func",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1fe7bfb3-498c-4bd9-a719-57c975cd72e6",
   "metadata": {
    "language": "sql",
    "name": "cell11",
    "collapsed": false
   },
   "outputs": [],
   "source": "/**************************\nLoads Articles that have not been processed yet into PMC_OA_CHUNKS_TABLE\nBatch processing 100 articles by using \"LIMIT 100\"\n**************************/\n\ninsert into PMC_OA_chunks_table (relative_path, abs_path,etag,scoped_file_url, chunk)\nwith oa_comm as \n(select array_to_string(array_slice(split(key, '/'),1,4), '/') as relative_path , etag \nfrom oa_comm_metadata where etag not in (select etag from PMC_OA_chunks_table ) limit 100)\nselect \n    relative_path, \n    get_absolute_path(@pmc_oa_comm_raw, relative_path) as abs_path,\n    etag,\n    build_scoped_file_url(@pmc_oa_comm_raw, relative_path) as scoped_file_url,\n    func.chunk as chunk\nfrom \n    oa_comm,\n    TABLE(text_chunker(build_scoped_file_url(@pmc_oa_comm_raw, relative_path))) as func;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f12f4f48-9eb4-4d78-9d3d-574e0badabb9",
   "metadata": {
    "language": "sql",
    "name": "cell12",
    "collapsed": false
   },
   "outputs": [],
   "source": "/**************************\nCreate a single view that combines the article metadata and the chunked text\n**************************/\n--View for Search service definition\ncreate or replace view pmc_service_vw as \n(select pmc_oa_chunks_table.etag, article_citation,accessionid, last_updated_utc, pmid, retracted, license, chunk  from pmc_oa_opendata.PMC_OA_chunks_table \nleft join  pmc_oa_opendata.oa_comm_metadata on pmc_oa_chunks_table.etag=oa_comm_metadata.etag);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d808c2b-34a9-488a-81d1-019dd123f5fd",
   "metadata": {
    "language": "sql",
    "name": "cell19",
    "collapsed": false
   },
   "outputs": [],
   "source": "---Pick PMC ID to chat with.\n\nSelect * from pmc_service_vw limit 10;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0c2d42a-be64-4bd0-b5c3-0739c4f29fc8",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "[Create Cortex Search Documentation](https://docs.snowflake.com/en/sql-reference/sql/create-cortex-search)\n\n[Embedding models available](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview#regional-availability)"
  },
  {
   "cell_type": "code",
   "id": "3400129e-2cb4-4347-88a6-833101d8394a",
   "metadata": {
    "language": "sql",
    "name": "cell13",
    "collapsed": false
   },
   "outputs": [],
   "source": "/**************************\nCreate a search service \n\n--UPDATE WAREHOUSE \n**************************/\nCreate or replace cortex search service my_pmc_search_service on chunk \nattributes etag, article_citation, accessionid, last_updated_utc, pmid, retracted, license \nwarehouse= fh_pmc_wh1\ntarget_lag= '1 min'\nEMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nas (\nselect etag, article_citation,accessionid, last_updated_utc, pmid, retracted, license, chunk from pmc_oa_opendata.pmc_service_vw\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e4f38a3-73d2-473f-ad42-0baf7caca9a4",
   "metadata": {
    "language": "sql",
    "name": "cell14",
    "collapsed": false
   },
   "outputs": [],
   "source": "Describe Cortex Search service my_pmc_search_service;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "70672585-61a2-4e4a-865b-d0ad76743442",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "**How to Query the Cortex Search Service**"
  },
  {
   "cell_type": "code",
   "id": "bb43ee3c-a848-4a09-90b6-1439d8af374f",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "collapsed": false
   },
   "outputs": [],
   "source": "## Import python packages\n##UPDATE DATABASE Line 11\n\nimport streamlit as st\nimport pandas as pd \nfrom snowflake.core import Root\nimport json\nimport pandas as pd\n\nroot = Root(session)\npmc_search_service = (root\n  .databases[\"FH_PMC_DATA1\"]\n  .schemas[\"PMC_OA_OPENDATA\"]\n  .cortex_search_services[\"my_pmc_search_service\"]\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "175f0b8a-d679-42bb-a419-af9ff168d336",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "collapsed": false
   },
   "outputs": [],
   "source": "#Pick an ID from the search service view\n#https://www.ncbi.nlm.nih.gov/pmc/?term=open%20access%5Bfilter%5D\n\nsimilar_articles=pmc_search_service.search(\n    query='article abstract',\n    columns= [\"CHUNK\", \"ACCESSIONID\"],\n    filter={\"@eq\": {\"accessionid\": \"PMC9975415\"} },\n    limit=10\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "593a5c64-d6cf-4c6b-aa36-91d917ae570e",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "collapsed": false
   },
   "outputs": [],
   "source": "similar_articles_resp=similar_articles.to_json()\ndata = json.loads(similar_articles_resp)\ndf = pd.json_normalize(data['results'])\ndf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3eb6bbc5-8a8b-4d85-9bdd-d06b120a25c6",
   "metadata": {
    "language": "python",
    "name": "cell21",
    "collapsed": false
   },
   "outputs": [],
   "source": "#example streamlit code \n#UPDATE DATABASE LINE 16\nimport streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session() # Get the current credentials\n\nimport pandas as pd\n\npd.set_option(\"max_colwidth\",None)\nnum_chunks = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy\n\ndef create_prompt (myquestion, rag, article_chosen=None):\n    st.write(article_chosen)\n    root = Root(session)\n    pmc_search_service = (root\n                .databases[\"FH_PMC_DATA1\"]\n                .schemas[\"PMC_OA_OPENDATA\"]\n                .cortex_search_services[\"my_pmc_search_service\"]\n                )\n    if rag == 1:  \n        if article_chosen:\n            similar_articles=pmc_search_service.search(\n                query='article abstract',\n                columns= [\"CHUNK\", \"ACCESSIONID\"],\n                filter={\"@eq\": {\"accessionid\": f\"{article_chosen}\" }},\n                limit=10\n                )\n            \n            similar_articles_resp=similar_articles.to_json()\n            data = json.loads(similar_articles_resp)\n            df_context = pd.json_normalize(data['results'])\n            st.write(df_context)\n        \n            \n            context_lenght = len(df_context) -1\n    \n            prompt_context = \"\"\n            for i in range (0, context_lenght):\n                prompt_context += df_context._get_value(i, 'CHUNK')\n    \n            prompt_context = prompt_context.replace(\"'\", \"\")\n            accessionid =  df_context._get_value(0,'ACCESSIONID')\n        \n            prompt = f\"\"\"\n              'You are an expert assistance extracting information from context provided. \n               Answer the question based on the context. Be concise and do not hallucinate. \n               If you don´t have the information just say so.\n              Context: {prompt_context}\n              Question:  \n               {myquestion} \n               Answer: '\n               \"\"\"\n        else:\n            similar_articles=pmc_search_service.search(\n                query='article abstract',\n                columns= [\"CHUNK\", \"ACCESSIONID\"],\n                limit=10\n                )\n            \n            similar_articles_resp=similar_articles.to_json()\n            data = json.loads(similar_articles_resp)\n            df_context = pd.json_normalize(data['results'])\n        \n            \n            context_lenght = len(df_context) -1\n    \n            prompt_context = \"\"\n            for i in range (0, context_lenght):\n                prompt_context += df_context._get_value(i, 'CHUNK')\n    \n            prompt_context = prompt_context.replace(\"'\", \"\")\n            accessionid =  df_context._get_value(0,'ACCESSIONID')\n        \n            prompt = f\"\"\"\n              'You are an expert assistance extracting information from context provided. \n               Answer the question based on the context. Be concise and do not hallucinate. \n               If you don´t have the information just say so.\n              Context: {prompt_context}\n              Question:  \n               {myquestion} \n               Answer: '\n               \"\"\"\n\n    else:\n        prompt = f\"\"\"\n         'Question:  \n           {myquestion} \n           Answer: '\n           \"\"\"\n        accessionid = \"None\"\n        \n        \n    return prompt,accessionid\n\ndef complete(myquestion, model_name, rag = 1, article_chosen=None):\n    #st.write(article_chosen)\n\n    prompt,accessionid =create_prompt (myquestion, rag, article_chosen)\n    cmd = f\"\"\"\n             select SNOWFLAKE.CORTEX.COMPLETE(?,?) as response\n           \"\"\"\n    \n    df_response = session.sql(cmd, params=[model_name, prompt]).collect()\n    return df_response, accessionid\n\ndef display_response (question, model, rag=0, article_chosen=None):\n    response, accessionid= complete(question, model, rag, article_chosen)\n    res_text = response[0].RESPONSE\n    st.markdown(res_text)\n    if rag == 1:\n        article_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{accessionid}/\"\n        text= f\"\\n\\nAssociated NCBI AccessionID that may be useful:\\n[{accessionid}]({article_url})\"\n        st.write(text)\n\n#Main code\n\nst.title(\"Chat with NCBI Pubmed Articles with Cortex LLMs:\")\nst.write(\"\"\"You can ask questions and decide if you want to the NCBI Articles for context or allow the model to create their own response.\"\"\")\n\ndocs_available = session.sql(\"select distinct ACCESSIONID from PMC_SERVICE_VW limit 10\").collect()\nlist_docs = []\nfor doc in docs_available:\n    list_docs.append(doc[\"ACCESSIONID\"])\n\nrag = st.checkbox('Use articles as context?')\n\nif rag:\n    use_rag = 1\n    article_chosen=st.selectbox(\"Choose an Article for Context\",list_docs)\nelse:\n    use_rag = 0\n    article_chosen=None\n\n#Here you can choose what LLM to use. Please note that they will have different cost & performance\nmodel = st.selectbox('Select your model:',('claude-3-5-sonnet',\n                                    'mixtral-8x7b',\n                                    'snowflake-arctic',\n                                    'mistral-large',\n                                    'llama3-8b',\n                                    'llama3-70b',\n                                    'reka-flash',\n                                     'mistral-7b',\n                                     'llama2-70b-chat',\n                                     'gemma-7b'))\n\nquestion = st.text_input(\"Enter question\", placeholder=\"Is there articles related to type-II diabetes?\", label_visibility=\"collapsed\")\n\nif question:\n    display_response (question, model, use_rag, article_chosen)",
   "execution_count": null
  }
 ]
}